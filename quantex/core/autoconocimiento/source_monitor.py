# quantex/pipelines/knowledge/source_monitor.py (Versi√≥n 14.0 - FXStreet Integrado)
import os
import sys
import time
import feedparser
import re
import json
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Tuple
# Imports eliminados: numpy, cosine_similarity - IA detecta duplicados sem√°nticos

# Desactivar logging de httpx para evitar spam de consultas HTTP
logging.getLogger("httpx").setLevel(logging.WARNING)

# --- L√≥gica de Rutas ---
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

# --- Importaciones de Quantex (Centralizadas) ---
from quantex.core import database_manager as db
from quantex.core.web_tools import get_firecrawl_scrape
from quantex.core.ai_services import ai_services
from quantex.core import agent_tools
from quantex.core.knowledge_graph.ingestion_engine import KnowledgeGraphIngestionEngine
from quantex.core.config_loader import get_config_loader
from quantex.core.llm_manager import generate_completion

# --- Clases de Screening Inteligente ---

class NewsScreeningAgent:
    """
    Agente de screening inteligente para evaluar relevancia de noticias
    """
    
    def __init__(self, commodity: str, keywords: List[str]):
        self.commodity = commodity
        self.keywords = keywords
        self.ai_service = ai_services
    
    def screen_with_full_context(self, title: str, summary: str, url: str, source: str,
                                feed_similarity: float, historical_similarity: float,
                                similar_titles: List[str], feed_context: List[str]) -> Dict[str, Any]:
        """
        Screening completo con contexto de duplicados
        """
        prompt = f"""
Eres un experto en an√°lisis de noticias financieras y de commodities. 
Eval√∫a la relevancia de esta noticia para el mercado de {self.commodity}.

CONTEXTO DEL FEED:
{feed_context[:5]}  # Primeras 5 noticias del feed

NOTICIA ACTUAL:
T√≠tulo: {title}
Resumen: {summary}
URL: {url}
Fuente: {source}

AN√ÅLISIS DE DUPLICADOS:
- Similitud con noticias del feed: {feed_similarity:.2f}
- Similitud con noticias hist√≥ricas: {historical_similarity:.2f}
- T√≠tulos similares encontrados: {similar_titles[:3]}

CRITERIOS DE EVALUACI√ìN:
1. **Novedad**: ¬øEsta noticia aporta informaci√≥n nueva?
2. **Relevancia**: ¬øEs relevante para {self.commodity}?
3. **Impacto**: ¬øPuede afectar precios o mercado?
4. **Calidad**: ¬øEs informaci√≥n confiable y detallada?

RESPUESTA REQUERIDA (JSON):
{{
    "relevant": true/false,
    "confidence": 0.0-1.0,
    "novelty_score": 0.0-1.0,
    "reasoning": "Explicaci√≥n breve",
    "impact_level": "high/medium/low",
    "duplicate_analysis": {{
        "is_duplicate": true/false,
        "similarity_level": "high/medium/low",
        "new_information": "Qu√© informaci√≥n nueva aporta"
    }},
    "recommended_action": "scrape/ignore/monitor"
}}
"""
        
        try:
            # Importar el LLM manager para usar la funci√≥n correcta
            from quantex.core.llm_manager import generate_completion
            
            # Usar generate_completion en lugar de chat_completion
            response = generate_completion(
                task_complexity="default",
                system_prompt="Eres un experto en an√°lisis de noticias financieras y de commodities.",
                user_prompt=prompt
            )
            
            # Parsear respuesta JSON desde raw_text
            if 'raw_text' in response:
                decision = json.loads(response['raw_text'])
                return decision
            else:
                raise Exception(f"Respuesta inv√°lida del LLM: {response}")
            
        except Exception as e:
            print(f"      ‚ùå Error en screening IA: {e}")
            # Fallback: usar filtro b√°sico
            return self.fallback_screening(title, summary)
    
    def fallback_screening(self, title: str, summary: str) -> Dict[str, Any]:
        """
        Screening de fallback cuando falla la IA
        """
        text = f"{title} {summary}".lower()
        keyword_matches = sum(1 for keyword in self.keywords if keyword.lower() in text)
        
        relevant = keyword_matches > 0
        confidence = min(keyword_matches / len(self.keywords), 1.0)
        
        return {
            "relevant": relevant,
            "confidence": confidence,
            "novelty_score": 0.5,
            "reasoning": f"Fallback: {keyword_matches} keywords encontradas",
            "impact_level": "medium",
            "duplicate_analysis": {
                "is_duplicate": False,
                "similarity_level": "low",
                "new_information": "An√°lisis b√°sico"
            },
            "recommended_action": "scrape" if relevant else "ignore"
        }
    
    def screen_with_full_content(self, title: str, summary: str, content: str, url: str, source: str,
                                feed_similarity: float, historical_similarity: float,
                                similar_titles: List[str], feed_context: List[str]) -> Dict[str, Any]:
        """
        Screening completo con contenido scrapeado
        """
        prompt = f"""
Eres un experto en an√°lisis de noticias financieras y de commodities. 
Eval√∫a la relevancia de esta noticia para el mercado de {self.commodity}.

CONTENIDO COMPLETO DE LA NOTICIA:
T√≠tulo: {title}
Resumen: {summary}
Contenido completo: {content[:2000]}...

URL: {url}
Fuente: {source}

AN√ÅLISIS DE DUPLICADOS:
- Similitud con noticias del feed: {feed_similarity:.2f}
- Similitud con noticias hist√≥ricas: {historical_similarity:.2f}
- T√≠tulos similares encontrados: {similar_titles[:3]}

CRITERIOS DE EVALUACI√ìN:
1. **Novedad**: ¬øEsta noticia aporta informaci√≥n nueva?
2. **Relevancia**: ¬øEs relevante para {self.commodity}?
3. **Impacto**: ¬øPuede afectar precios o mercado?
4. **Calidad**: ¬øEs informaci√≥n confiable y detallada?

RESPUESTA REQUERIDA (JSON):
{{
    "relevant": true/false,
    "confidence": 0.0-1.0,
    "novelty_score": 0.0-1.0,
    "reasoning": "Explicaci√≥n breve basada en el contenido completo",
    "impact_level": "high/medium/low",
    "duplicate_analysis": {{
        "is_duplicate": true/false,
        "similarity_level": "high/medium/low",
        "new_information": "Qu√© informaci√≥n nueva aporta"
    }},
    "recommended_action": "scrape/ignore/monitor"
}}
"""
        
        try:
            response = generate_completion(
                task_complexity="default",
                system_prompt="Eres un experto en an√°lisis de noticias financieras y de commodities.",
                user_prompt=prompt
            )
            
            if 'raw_text' in response:
                try:
                    raw_text = response['raw_text'].strip()
                    if '{' in raw_text and '}' in raw_text:
                        start = raw_text.find('{')
                        end = raw_text.rfind('}') + 1
                        json_text = raw_text[start:end]
                        decision = json.loads(json_text)
                        return decision
                    else:
                        raise Exception("No se encontr√≥ JSON v√°lido en la respuesta")
                except json.JSONDecodeError as e:
                    print(f"      ‚ö†Ô∏è Error parseando JSON: {e}")
                    print(f"      üìù Respuesta raw: {response['raw_text'][:200]}...")
                    raise Exception(f"Error parseando JSON: {e}")
            else:
                raise Exception(f"Respuesta inv√°lida del LLM: {response}")
            
        except Exception as e:
            print(f"      ‚ùå Error en screening IA: {e}")
            return self.fallback_screening(title, summary)

# Funci√≥n eliminada: calculate_semantic_similarity - IA detecta duplicados sem√°nticos

def enhanced_prefilter(title: str, summary: str, keywords: List[str], filter_config: dict = None) -> bool:
    """
    Prefiltro mejorado con configuraci√≥n desde YAML
    """
    text = f"{title} {summary}".lower()
    
    # Si no hay configuraci√≥n, usar filtro b√°sico
    if not filter_config:
        keyword_matches = sum(1 for keyword in keywords if keyword.lower() in text)
        return keyword_matches > 0
    
    # Obtener configuraci√≥n del filtro
    specific_keywords = [kw.lower() for kw in filter_config.get('specific_keywords', [])]
    general_keywords = [kw.lower() for kw in filter_config.get('general_keywords', [])]
    exclude_keywords = [kw.lower() for kw in filter_config.get('exclude_keywords', [])]
    target_pairs = [pair.lower() for pair in filter_config.get('target_pairs', [])]
    filter_logic = filter_config.get('filter_logic', 'any_keyword')
    
    # 1. Verificar exclusiones primero
    if exclude_keywords:
        for exclude in exclude_keywords:
            if exclude in text:
                print(f"      ‚ùå Excluido por keyword: {exclude}")
                return False
    
    # 2. Verificar pares de divisas espec√≠ficos
    if target_pairs:
        for pair in target_pairs:
            if pair in text:
                print(f"      ‚úÖ Aprobado por par espec√≠fico: {pair}")
                return True
    
    # 3. Aplicar l√≥gica del filtro
    if filter_logic == "specific_or_general":
        # Debe tener al menos una divisa espec√≠fica O ser forex general
        has_specific = any(currency in text for currency in specific_keywords)
        has_general = any(general in text for general in general_keywords)
        result = has_specific or has_general
        if result:
            print(f"      ‚úÖ Aprobado por filtro espec√≠fico/general")
        return result
    
    elif filter_logic == "all_specific":
        # Debe tener TODAS las keywords espec√≠ficas
        result = all(currency in text for currency in specific_keywords)
        if result:
            print(f"      ‚úÖ Aprobado por todas las keywords espec√≠ficas")
        return result
    
    elif filter_logic == "any_keyword":
        # Cualquier keyword de la lista
        all_keywords = specific_keywords + general_keywords
        result = any(keyword in text for keyword in all_keywords)
        if result:
            print(f"      ‚úÖ Aprobado por cualquier keyword")
        return result
    
    else:
        # Fallback: filtro b√°sico
        keyword_matches = sum(1 for keyword in keywords if keyword.lower() in text)
        return keyword_matches > 0

# Funci√≥n eliminada: analyze_feed_context - IA detecta duplicados sem√°nticos

# Funci√≥n eliminada: check_historical_duplicates
# Ahora usamos solo verificaci√≥n de URL exacta para mayor eficiencia

def does_url_exist_in_db(url: str) -> bool:
    """
    Verifica si una URL espec√≠fica ya existe en la base de datos (m√©todo 1 a 1)
    """
    try:
        if not url:
            return False
            
        # Query que funciona para verificar URL individual
        response = db.supabase.table('nodes') \
            .select('id') \
            .eq('type', 'Documento') \
            .eq('properties->>original_url', url) \
            .limit(1) \
            .execute()
        
        exists = len(response.data) > 0
        return exists
        
    except Exception as e:
        print(f"      ‚ö†Ô∏è Error verificando URL individual: {e}")
        return False

def get_existing_urls_batch(urls: List[str]) -> set:
    """
    (Versi√≥n Optimizada - Consulta Masiva Alternativa)
    Obtiene todas las URLs existentes en una sola consulta, filtrando en Python
    """
    try:
        if not urls:
            return set()
            
        # Query que S√ç funciona (con filtro NOT NULL)
        response = db.supabase.table('nodes') \
            .select('properties->>original_url') \
            .eq('type', 'Documento') \
            .not_.is_('properties->>original_url', 'null') \
            .execute()
        
        # Crear set de todas las URLs existentes
        all_existing_urls = set()
        for row in response.data:
            if row.get('original_url'):
                all_existing_urls.add(row['original_url'])
        
        print(f"      -> üîç URLs encontradas en DB: {len(all_existing_urls)}")
        
        # Filtrar solo las que est√°n en el feed actual
        existing_urls = {url for url in all_existing_urls if url in urls}
        
        print(f"      -> üìä Consulta masiva: {len(existing_urls)} URLs encontradas en DB de {len(urls)} del feed")
        
        # Debug: mostrar URLs duplicadas
        if existing_urls:
            print(f"      -> üîç URLs duplicadas encontradas:")
            for i, url in enumerate(list(existing_urls)[:3]):
                print(f"         {i+1}. {url[:80]}...")
        print(f"      -> üìã Total documentos en DB: {len(all_existing_urls)}")
        
        return existing_urls
        
    except Exception as e:
        print(f"      ‚ö†Ô∏è Error en consulta masiva de URLs: {e}")
        return set()

def process_rss_feed(target: dict, ingestion_engine: KnowledgeGraphIngestionEngine):
    """
    (Versi√≥n 16.0 - Optimizado)
    Procesa un feed con eliminaci√≥n de duplicados en lote para m√°xima eficiencia.
    """
    target_name = target.get('target_name', 'Desconocido')
    rss_url = target.get('source_url')
    
    print(f"\n--- üì∞ Procesando Feed: {target_name} ---")
    if not rss_url: return

    # Obtener keywords del target
    filter_keywords = target.get('filter_keywords', [])
    commodity = target.get('commodity', 'copper')
    
    try:
        # 1. Cargar RSS feed completo
        feed = feedparser.parse(rss_url)
        if not feed.entries:
            print("  -> No se encontraron noticias en este feed.")
            return

        print(f"  üì∞ Feed cargado: {len(feed.entries)} noticias encontradas")
        
        # 2. Crear agente de screening
        screening_agent = NewsScreeningAgent(commodity, filter_keywords)
        
        # 3. Contadores para estad√≠sticas
        new_articles_processed = 0
        skipped_prefilter = 0
        skipped_screening = 0
        skipped_existing = 0

        # 4. Procesar cada noticia individualmente (m√©todo 1 a 1 que funciona)
        for entry in feed.entries:
            entry_link = entry.get('link')
            title = entry.get('title', '')
            summary = entry.get('summary', '')

            # 4.1 Verificar si URL ya existe en DB (m√©todo 1 a 1)
            if entry_link:
                if does_url_exist_in_db(entry_link):
                    skipped_existing += 1
                    continue

            # 4.2 Prefiltro mejorado con keywords
            filter_config = target.get('filter_config')
            if not enhanced_prefilter(title, summary, target.get('filter_keywords', []), filter_config):
                skipped_prefilter += 1
                continue
            
            # Solo mostrar noticias que pasan el prefiltro
            print(f"    üì∞ Procesando: '{title[:60]}...'")
            
            # 4.3 Proceder con scrape (antes del screening)
            full_content_md = scrape_article_with_retries(entry_link)
            if not full_content_md:
                print(f"      ‚è≠Ô∏è  Sin contenido tras scrapeo")
                continue
            
            # 4.4 OMITIR screening IA con contenido completo (pol√≠tica actual)
            print("      ‚ö†Ô∏è Screening IA omitido por pol√≠tica: se proceder√° a ingesta tras prefiltro.")
            screening_result = {
                "relevant": True,
                "confidence": 1.0,
                "novelty_score": 0.0,
                "impact_level": "unclassified"
            }
            
            # 4.6 Proceder con ingesta
            print(f"      ‚úÖ Aprobado: {screening_result['impact_level']} impact")
            new_articles_processed += 1
            
            try:
                source_context = {
                    "source": target.get('publisher', target_name),
                    "topic": target.get('target_name'),
                    "source_type": "Noticia Continua",
                    "original_url": entry_link,
                    "screening_score": screening_result['confidence'],
                    "novelty_score": screening_result['novelty_score'],
                    "impact_level": screening_result['impact_level'],
                    "feed_similarity": 0.0,  # Eliminado: IA detecta duplicados
                    "historical_similarity": 0.0  # Simplificado: solo URL exacta
                }
                
                # Usar el nuevo motor de ingesta centralizado
                result = ingestion_engine.ingest_document(full_content_md, source_context)
                if result.get("success"):
                    print(f"      -> ‚úÖ {result.get('nodes_created', 0)} nodo(s) creado(s) con conexiones sem√°nticas.")
                else:
                    print(f"      -> ‚ùå Error en ingesta: {result.get('reason', 'Desconocido')}")

            except Exception as e:
                print(f"      -> ‚ùå Error en el procesamiento profundo de la noticia '{entry_link}': {e}")
        
        # 5. Mostrar resumen de procesamiento
        print(f"\n  üìä Resumen: {new_articles_processed} procesadas, {skipped_existing} duplicadas, {skipped_prefilter} prefiltradas, {skipped_screening} rechazadas")
        
        if new_articles_processed == 0:
            print("  ‚úÖ No se encontraron noticias nuevas para procesar.")
        else:
            print(f"  ‚úÖ Se procesaron {new_articles_processed} noticias nuevas de este feed.")

    except Exception as e:
        print(f"  -> ‚ùå Error fatal al procesar el feed '{target_name}': {e}")


def run_automated_monitoring():
    print("\n--- Vigilancia Automatica de RSS (Version 16.0 - Optimizado) ---")
    
    # Inicializar el nuevo motor de ingesta centralizado
    print("Inicializando Motor de Ingesta Centralizado...")
    ingestion_engine = KnowledgeGraphIngestionEngine()
    
    # Cargar configuraci√≥n desde YAML
    print("Cargando configuracion de fuentes desde YAML...")
    config_loader = get_config_loader()
    
    try:
        # Obtener fuentes RSS activas desde YAML
        targets = config_loader.get_rss_sources(active_only=True)
        if not targets:
            print("-> No se encontraron objetivos RSS activos para procesar.")
            return
        
        print(f"-> Se encontraron {len(targets)} fuentes RSS activas.")
        
        for target in targets:
            process_rss_feed(target, ingestion_engine)
            # Timestamps eliminados del YAML para simplificar el sistema
            
    except Exception as e:
        print(f"‚ùå ERROR CR√çTICO durante la vigilancia de RSS: {e}")

# Funci√≥n eliminada: process_fxstreet_feed
# Ahora FXStreet se procesa desde YAML como las otras fuentes

def clean_scraped_markdown(markdown_text: str) -> str:
    print("      -> üßπ Limpiando contenido scrapeado...")
    text = markdown_text
    match = re.search(r'# .*', text)
    if match: text = text[match.start():]
    footer_start_points = ["Data Source Statement:", "SMM Events & Webinars", "MOST POPULAR"]
    for start_point in footer_start_points:
        if start_point in text: text = text[:text.find(start_point)]; break
    text = re.sub(r'!\[.*?\]\(.*?\)', '', text)
    text = re.sub(r'\n\s*\n', '\n\n', text)
    print("      -> ‚úÖ Limpieza completada.")
    return text.strip()

def scrape_article_with_retries(url: str) -> str:
    print(f"      -> üî• Realizando scrapeo profundo de: {url}")
    for i in range(3):
        try:
            print(f"        -> Intento {i + 1} de 3...")
            scraped_data = get_firecrawl_scrape(url) 
            if scraped_data and isinstance(scraped_data, dict):
                markdown_content = scraped_data.get('markdown', '')
                if markdown_content:
                    return clean_scraped_markdown(markdown_content)
        except Exception as e:
            print(f"      -> ‚ùå Fallo en el intento {i + 1}: {e}")
            if i < 2: time.sleep(5)
    print("      -> ‚ùå Se han agotado todos los intentos de scrapeo.")
    return ""

# --- C√ìDIGO DE ARRANQUE ---
if __name__ == '__main__':
    print("\n--- Iniciando Vigilante de Fuentes (v11.0 Metodo Robusto) ---")
    ai_services.initialize()
    run_automated_monitoring()
    print("\n--- Vigilante ha finalizado su ronda. ---")